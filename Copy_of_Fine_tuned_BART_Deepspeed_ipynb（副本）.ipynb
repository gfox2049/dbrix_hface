{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa47e0f80ea3427eb6c4de0db7e98314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_28c279c001af4691946e698dab62d4c4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fe6d4ed4f361478c912c0a935485119e",
              "IPY_MODEL_f0a10bd7b6334485a728bdaab889057a",
              "IPY_MODEL_00eacc960c4a4d199088f2b238c467a3"
            ]
          }
        },
        "28c279c001af4691946e698dab62d4c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fe6d4ed4f361478c912c0a935485119e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3dbcc45ea92048eba5de012a6e74fb0a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_945becf7d1c94c209ac09d67105c48bd"
          }
        },
        "f0a10bd7b6334485a728bdaab889057a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9e75ba8660924cf8b102d12c7c3018a5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1635,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1635,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddcb3417c1764dc98716690babd4c403"
          }
        },
        "00eacc960c4a4d199088f2b238c467a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2a085688c6e6494a9779c21159fe9da3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.64k/1.64k [00:00&lt;00:00, 52.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b8ae1f5d2f2d4ac096deab02d80cdaf7"
          }
        },
        "3dbcc45ea92048eba5de012a6e74fb0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "945becf7d1c94c209ac09d67105c48bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9e75ba8660924cf8b102d12c7c3018a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddcb3417c1764dc98716690babd4c403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "2a085688c6e6494a9779c21159fe9da3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b8ae1f5d2f2d4ac096deab02d80cdaf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "760a4915b5814f099f65284f7dca778d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8be67e6b9dac4aafa98d479d889ca750",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_907cec2dd19a420bbfb989cac8a869c7",
              "IPY_MODEL_eb94f18d22ad4aaaa871ba1e5c9ab298",
              "IPY_MODEL_f20433a131d44eebbf417d5a523b2d3a"
            ]
          }
        },
        "8be67e6b9dac4aafa98d479d889ca750": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "907cec2dd19a420bbfb989cac8a869c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba35cc2b6ae045d9bd9a4129a93100a9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fe56386302ba4f05948999439d2932e2"
          }
        },
        "eb94f18d22ad4aaaa871ba1e5c9ab298": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8c169934f016401089346319f5523121",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1083777,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1083777,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7924d7449346465aa8e7a9855108b51f"
          }
        },
        "f20433a131d44eebbf417d5a523b2d3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4aeec6e32c8943e1b5df9def99d58968",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.08M/1.08M [00:00&lt;00:00, 3.04MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3d09d777a31c4d399b30d6f40ae66ad5"
          }
        },
        "ba35cc2b6ae045d9bd9a4129a93100a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fe56386302ba4f05948999439d2932e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8c169934f016401089346319f5523121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7924d7449346465aa8e7a9855108b51f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4aeec6e32c8943e1b5df9def99d58968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3d09d777a31c4d399b30d6f40ae66ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vUZYKjiHi-g",
        "outputId": "a4ff1c0c-2831-4559-aac8-28452779b517"
      },
      "source": [
        "!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.9.0+cu111\n",
            "  Downloading https://download.pytorch.org/whl/cu111/torch-1.9.0%2Bcu111-cp37-cp37m-linux_x86_64.whl (2041.3 MB)\n",
            "\u001b[K     |█████████████                   | 834.1 MB 1.3 MB/s eta 0:15:52tcmalloc: large alloc 1147494400 bytes == 0x55ce9da76000 @  0x7fb99d8d7615 0x55ce6417c02c 0x55ce6425c17a 0x55ce6417ee4d 0x55ce64270c0d 0x55ce641f30d8 0x55ce641edc35 0x55ce6418073a 0x55ce641f2f40 0x55ce641edc35 0x55ce6418073a 0x55ce641ef93b 0x55ce64271a56 0x55ce641eefb3 0x55ce64271a56 0x55ce641eefb3 0x55ce64271a56 0x55ce641eefb3 0x55ce64180b99 0x55ce641c3e79 0x55ce6417f7b2 0x55ce641f2e65 0x55ce641edc35 0x55ce6418073a 0x55ce641ef93b 0x55ce641edc35 0x55ce6418073a 0x55ce641eeb0e 0x55ce6418065a 0x55ce641eed67 0x55ce641edc35\n",
            "\u001b[K     |████████████████▌               | 1055.7 MB 80.2 MB/s eta 0:00:13tcmalloc: large alloc 1434370048 bytes == 0x55cee20cc000 @  0x7fb99d8d7615 0x55ce6417c02c 0x55ce6425c17a 0x55ce6417ee4d 0x55ce64270c0d 0x55ce641f30d8 0x55ce641edc35 0x55ce6418073a 0x55ce641f2f40 0x55ce641edc35 0x55ce6418073a 0x55ce641ef93b 0x55ce64271a56 0x55ce641eefb3 0x55ce64271a56 0x55ce641eefb3 0x55ce64271a56 0x55ce641eefb3 0x55ce64180b99 0x55ce641c3e79 0x55ce6417f7b2 0x55ce641f2e65 0x55ce641edc35 0x55ce6418073a 0x55ce641ef93b 0x55ce641edc35 0x55ce6418073a 0x55ce641eeb0e 0x55ce6418065a 0x55ce641eed67 0x55ce641edc35\n",
            "\u001b[K     |███████████████████▌            | 1247.2 MB 1.2 MB/s eta 0:10:38"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Rl4dAayK8kw"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDyTFesatPG_"
      },
      "source": [
        "! pip install datasets transformers rouge-score nltk deepspeed\n",
        "import nltk\n",
        "from datasets import load_from_disk\n",
        "from datasets import Dataset\n",
        "import datasets\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig, PretrainedConfig, AutoModel, AutoModelForPreTraining, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from transformers import DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_Qj4U86dyHO"
      },
      "source": [
        "os.environ['MASTER_ADDR'] = 'localhost'\n",
        "os.environ['MASTER_PORT'] = '9994' # modify if RuntimeError: Address already in use\n",
        "os.environ['RANK'] = \"0\"\n",
        "os.environ['LOCAL_RANK'] = \"0\"\n",
        "os.environ['WORLD_SIZE'] = \"1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNfaWwBHQUQG"
      },
      "source": [
        "# !git clone https://github.com/microsoft/DeepSpeed/\n",
        "# !cd DeepSpeed\n",
        "# !rm -rf build\n",
        "# !TORCH_CUDA_ARCH_LIST=\"6.0\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n",
        "# !--global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v \\\n",
        "# !--disable-pip-version-check 2>&1 | tee build.log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inz4nSc-OQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aff673c-7eba-407c-d835-787d456b687e"
      },
      "source": [
        "!huggingface-cli login\n",
        "!pip install hf-lfs\n",
        "!git config --global user.email \"qichang.zheng18@student.xjtlu.edu.cn\"\n",
        "!git config --global user.name \"QichangZheng\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "        _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "        _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "        _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "        _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "        _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "        \n",
            "Username: qichang.zheng18@student.xjtlu.edu.cn\n",
            "Password: \n",
            "Login successful\n",
            "Your token: MJiGCmOowBrkkXnCwLRcvopDGWWgfWvpVjUAkTVPaRSVgzYiRhffKAPIFwWGLqsOqSkhCOfDdtFzTnDrklzaGRBECwFfArdeROvFkneSoXKJHNBTqIYoqubMiNvjhLTV \n",
            "\n",
            "Your token has been saved to /root/.huggingface/token\n",
            "Requirement already satisfied: hf-lfs in /usr/local/lib/python3.7/dist-packages (0.0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qprJoV5ckycR"
      },
      "source": [
        "nltk.download('punkt')\n",
        "model_checkpoint = 'facebook/bart-large-cnn'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRK8wcdLwKlU"
      },
      "source": [
        "# raw_datasets = load_from_disk(\"/content/drive/MyDrive/Patent_Dataset/Test_dataset_with_validation\")\n",
        "raw_datasets = load_from_disk(\"/content/drive/MyDrive/Patent_Dataset/Small_dataset\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz0Lvx1QPM8q"
      },
      "source": [
        "# config = PretrainedConfig(name_or_path=model_checkpoint, min_length=0, max_length=1024, early_stopping=True, num_beams=4)#min_length=0, max_length=1024, num_beams=2, length_penalty=1, early_stopping=True\n",
        "# print(type(config))\n",
        "# tokenizer = BartTokenizer.from_pretrained(model_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "# model = AutoModelForPreTraining.from_pretrained(model_checkpoint)\n",
        "# model = BartForConditionalGeneration.from_pretrained(model_checkpoint, min_length=0, max_length=1024, early_stopping=True)\n",
        "# model = AutoModel.from_pretrained(model_checkpoint)#, min_length=0, max_length=1024, early_stopping=True\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, min_length=0, max_length=1024, early_stopping=True)\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrHzKER8Exps"
      },
      "source": [
        "max_input_length = 1024\n",
        "max_target_length = 1024\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "    # Setup the tokenizer for targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUlR_5G9NnzC"
      },
      "source": [
        "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qurNJqS9i5HR"
      },
      "source": [
        "## **Fine-tune Process**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mme0FTOjTkAo"
      },
      "source": [
        "%%bash\n",
        "cat <<'EOT' > ds_config_zero3.json\n",
        "{\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"loss_scale\": 0,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"initial_scale_power\": 16,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "            \"lr\": \"auto\",\n",
        "            \"betas\": \"auto\",\n",
        "            \"eps\": \"auto\",\n",
        "            \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"WarmupLR\",\n",
        "        \"params\": {\n",
        "            \"warmup_min_lr\": \"auto\",\n",
        "            \"warmup_max_lr\": \"auto\",\n",
        "            \"warmup_num_steps\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 1e9,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 1e9,\n",
        "        \"stage3_max_reuse_distance\": 1e9,\n",
        "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
        "    },\n",
        "\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": \"auto\",\n",
        "    \"steps_per_print\": 2000,\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}\n",
        "EOT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQrUOhRxjegF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f927ab7-b9ef-49ab-a995-2fc8a8ce65ac"
      },
      "source": [
        "batch_size = 1\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Transformers/Output',\n",
        "    resume_from_checkpoint=False,\n",
        "    overwrite_output_dir=True,\n",
        "    eval_steps=1000000,\n",
        "    evaluation_strategy = \"steps\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.001,\n",
        "    save_total_limit=3,\n",
        "    save_steps=20000,\n",
        "    logging_steps=50000,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    # push_to_hub=False,\n",
        "    # push_to_hub_model_id=f\"{model_name}-finetuned-xsum\",\n",
        "    deepspeed=\"ds_config_zero3.json\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2021-08-18 18:15:45,081] [INFO] [distributed.py:47:init_distributed] Initializing torch distributed with backend: nccl\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C87shvnlkmtF"
      },
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w24KBnknkq8E"
      },
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    # decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # # Replace -100 in the labels as we can't decode them.\n",
        "    # labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    # decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    \n",
        "    # # Rouge expects a newline after each sentence\n",
        "    # decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    # decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "    \n",
        "    # result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # # Extract a few results\n",
        "    # result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    \n",
        "    # # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result = np.mean(prediction_lens)\n",
        "    \n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIv75IWhk5Vf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6f0219a-93f1-4ad4-dd17-2cdd5cf4b32b"
      },
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer\n",
        "    # compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using amp fp16 backend\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLof5Qv7JUth"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dWB3PqwLk7-E",
        "outputId": "96b29f98-a87f-43be-822a-e5e4f5f0a8e5"
      },
      "source": [
        "trainer.train()\n",
        "# trainer.train(from_checkpoint=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: summary, id, document.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-08-18 18:15:50,169] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed info: version=0.5.0, git-hash=unknown, git-branch=unknown\n",
            "[2021-08-18 18:16:00,825] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed groups\n",
            "[2021-08-18 18:16:00,826] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed model parallel group with size 1\n",
            "[2021-08-18 18:16:00,829] [INFO] [logging.py:68:log_dist] [Rank 0] initializing deepspeed expert parallel group with size 1\n",
            "[2021-08-18 18:16:00,832] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert data parallel process group with ranks: [0]\n",
            "[2021-08-18 18:16:00,834] [INFO] [logging.py:68:log_dist] [Rank 0] creating expert parallel process group with ranks: [0]\n",
            "[2021-08-18 18:16:00,897] [INFO] [engine.py:189:__init__] DeepSpeed Flops Profiler Enabled: False\n",
            "Installed CUDA version 11.0 does not match the version torch was compiled with 11.1 but since the APIs are compatible, accepting this combination\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Detected CUDA files, patching ldflags\n",
            "Emitting ninja build file /root/.cache/torch_extensions/cpu_adam/build.ninja...\n",
            "Building extension module cpu_adam...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module cpu_adam...\n",
            "Time to load cpu_adam op: 1.3162150382995605 seconds\n",
            "[2021-08-18 18:16:04,063] [INFO] [engine.py:791:_configure_optimizer] Using DeepSpeed Optimizer param name adamw as basic optimizer\n",
            "[2021-08-18 18:16:04,126] [INFO] [engine.py:797:_configure_optimizer] DeepSpeed Basic Optimizer = DeepSpeedCPUAdam\n",
            "[2021-08-18 18:16:04,127] [INFO] [utils.py:44:is_zero_supported_optimizer] Checking ZeRO support for optimizer=DeepSpeedCPUAdam type=<class 'deepspeed.ops.adam.cpu_adam.DeepSpeedCPUAdam'>\n",
            "[2021-08-18 18:16:04,128] [INFO] [logging.py:68:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer\n",
            "Initializing ZeRO Stage 3\n",
            "[2021-08-18 18:16:04,139] [INFO] [stage3.py:638:__init__] Reduce bucket size 1048576\n",
            "[2021-08-18 18:16:04,140] [INFO] [stage3.py:639:__init__] Allgather bucket size 943718.4\n",
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "Emitting ninja build file /root/.cache/torch_extensions/utils/build.ninja...\n",
            "Building extension module utils...\n",
            "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.705775260925293 seconds\n",
            "[2021-08-18 18:16:08,839] [INFO] [stage3.py:830:__init__] optimizer state initialized\n",
            "[2021-08-18 18:16:09,104] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed Final Optimizer = adamw\n",
            "[2021-08-18 18:16:09,105] [INFO] [engine.py:536:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
            "[2021-08-18 18:16:09,107] [INFO] [logging.py:68:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7fd77a368e10>\n",
            "[2021-08-18 18:16:09,109] [INFO] [logging.py:68:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[[0.9, 0.999]]\n",
            "[2021-08-18 18:16:09,110] [INFO] [config.py:932:print] DeepSpeedEngine configuration:\n",
            "[2021-08-18 18:16:09,112] [INFO] [config.py:936:print]   activation_checkpointing_config  {\n",
            "    \"partition_activations\": false, \n",
            "    \"contiguous_memory_optimization\": false, \n",
            "    \"cpu_checkpointing\": false, \n",
            "    \"number_checkpoints\": null, \n",
            "    \"synchronize_checkpoint_boundary\": false, \n",
            "    \"profile\": false\n",
            "}\n",
            "[2021-08-18 18:16:09,113] [INFO] [config.py:936:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
            "[2021-08-18 18:16:09,114] [INFO] [config.py:936:print]   allreduce_always_fp32 ........ False\n",
            "[2021-08-18 18:16:09,116] [INFO] [config.py:936:print]   amp_enabled .................. False\n",
            "[2021-08-18 18:16:09,117] [INFO] [config.py:936:print]   amp_params ................... False\n",
            "[2021-08-18 18:16:09,118] [INFO] [config.py:936:print]   checkpoint_tag_validation_enabled  True\n",
            "[2021-08-18 18:16:09,120] [INFO] [config.py:936:print]   checkpoint_tag_validation_fail  False\n",
            "[2021-08-18 18:16:09,121] [INFO] [config.py:936:print]   curriculum_enabled ........... False\n",
            "[2021-08-18 18:16:09,122] [INFO] [config.py:936:print]   curriculum_params ............ False\n",
            "[2021-08-18 18:16:09,123] [INFO] [config.py:936:print]   disable_allgather ............ False\n",
            "[2021-08-18 18:16:09,124] [INFO] [config.py:936:print]   dump_state ................... False\n",
            "[2021-08-18 18:16:09,126] [INFO] [config.py:936:print]   dynamic_loss_scale_args ...... {'init_scale': 65536, 'scale_window': 1000, 'delayed_shift': 2, 'min_scale': 1}\n",
            "[2021-08-18 18:16:09,127] [INFO] [config.py:936:print]   eigenvalue_enabled ........... False\n",
            "[2021-08-18 18:16:09,128] [INFO] [config.py:936:print]   eigenvalue_gas_boundary_resolution  1\n",
            "[2021-08-18 18:16:09,129] [INFO] [config.py:936:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
            "[2021-08-18 18:16:09,131] [INFO] [config.py:936:print]   eigenvalue_layer_num ......... 0\n",
            "[2021-08-18 18:16:09,132] [INFO] [config.py:936:print]   eigenvalue_max_iter .......... 100\n",
            "[2021-08-18 18:16:09,133] [INFO] [config.py:936:print]   eigenvalue_stability ......... 1e-06\n",
            "[2021-08-18 18:16:09,134] [INFO] [config.py:936:print]   eigenvalue_tol ............... 0.01\n",
            "[2021-08-18 18:16:09,136] [INFO] [config.py:936:print]   eigenvalue_verbose ........... False\n",
            "[2021-08-18 18:16:09,137] [INFO] [config.py:936:print]   elasticity_enabled ........... False\n",
            "[2021-08-18 18:16:09,138] [INFO] [config.py:936:print]   flops_profiler_config ........ {\n",
            "    \"enabled\": false, \n",
            "    \"profile_step\": 1, \n",
            "    \"module_depth\": -1, \n",
            "    \"top_modules\": 1, \n",
            "    \"detailed\": true, \n",
            "    \"output_file\": null\n",
            "}\n",
            "[2021-08-18 18:16:09,139] [INFO] [config.py:936:print]   fp16_enabled ................. True\n",
            "[2021-08-18 18:16:09,140] [INFO] [config.py:936:print]   fp16_master_weights_and_gradients  False\n",
            "[2021-08-18 18:16:09,141] [INFO] [config.py:936:print]   fp16_mixed_quantize .......... False\n",
            "[2021-08-18 18:16:09,143] [INFO] [config.py:936:print]   global_rank .................. 0\n",
            "[2021-08-18 18:16:09,144] [INFO] [config.py:936:print]   gradient_accumulation_steps .. 1\n",
            "[2021-08-18 18:16:09,145] [INFO] [config.py:936:print]   gradient_clipping ............ 1.0\n",
            "[2021-08-18 18:16:09,146] [INFO] [config.py:936:print]   gradient_predivide_factor .... 1.0\n",
            "[2021-08-18 18:16:09,149] [INFO] [config.py:936:print]   initial_dynamic_scale ........ 65536\n",
            "[2021-08-18 18:16:09,151] [INFO] [config.py:936:print]   loss_scale ................... 0\n",
            "[2021-08-18 18:16:09,154] [INFO] [config.py:936:print]   memory_breakdown ............. False\n",
            "[2021-08-18 18:16:09,155] [INFO] [config.py:936:print]   optimizer_legacy_fusion ...... False\n",
            "[2021-08-18 18:16:09,156] [INFO] [config.py:936:print]   optimizer_name ............... adamw\n",
            "[2021-08-18 18:16:09,157] [INFO] [config.py:936:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.001}\n",
            "[2021-08-18 18:16:09,158] [INFO] [config.py:936:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
            "[2021-08-18 18:16:09,159] [INFO] [config.py:936:print]   pld_enabled .................. False\n",
            "[2021-08-18 18:16:09,160] [INFO] [config.py:936:print]   pld_params ................... False\n",
            "[2021-08-18 18:16:09,161] [INFO] [config.py:936:print]   prescale_gradients ........... False\n",
            "[2021-08-18 18:16:09,163] [INFO] [config.py:936:print]   quantize_change_rate ......... 0.001\n",
            "[2021-08-18 18:16:09,165] [INFO] [config.py:936:print]   quantize_groups .............. 1\n",
            "[2021-08-18 18:16:09,168] [INFO] [config.py:936:print]   quantize_offset .............. 1000\n",
            "[2021-08-18 18:16:09,172] [INFO] [config.py:936:print]   quantize_period .............. 1000\n",
            "[2021-08-18 18:16:09,174] [INFO] [config.py:936:print]   quantize_rounding ............ 0\n",
            "[2021-08-18 18:16:09,175] [INFO] [config.py:936:print]   quantize_start_bits .......... 16\n",
            "[2021-08-18 18:16:09,176] [INFO] [config.py:936:print]   quantize_target_bits ......... 8\n",
            "[2021-08-18 18:16:09,178] [INFO] [config.py:936:print]   quantize_training_enabled .... False\n",
            "[2021-08-18 18:16:09,180] [INFO] [config.py:936:print]   quantize_type ................ 0\n",
            "[2021-08-18 18:16:09,181] [INFO] [config.py:936:print]   quantize_verbose ............. False\n",
            "[2021-08-18 18:16:09,182] [INFO] [config.py:936:print]   scheduler_name ............... WarmupLR\n",
            "[2021-08-18 18:16:09,183] [INFO] [config.py:936:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}\n",
            "[2021-08-18 18:16:09,184] [INFO] [config.py:936:print]   sparse_attention ............. None\n",
            "[2021-08-18 18:16:09,193] [INFO] [config.py:936:print]   sparse_gradients_enabled ..... False\n",
            "[2021-08-18 18:16:09,194] [INFO] [config.py:936:print]   steps_per_print .............. 2000\n",
            "[2021-08-18 18:16:09,195] [INFO] [config.py:936:print]   tensorboard_enabled .......... False\n",
            "[2021-08-18 18:16:09,197] [INFO] [config.py:936:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
            "[2021-08-18 18:16:09,198] [INFO] [config.py:936:print]   tensorboard_output_path ...... \n",
            "[2021-08-18 18:16:09,199] [INFO] [config.py:936:print]   train_batch_size ............. 1\n",
            "[2021-08-18 18:16:09,200] [INFO] [config.py:936:print]   train_micro_batch_size_per_gpu  1\n",
            "[2021-08-18 18:16:09,202] [INFO] [config.py:936:print]   use_quantizer_kernel ......... False\n",
            "[2021-08-18 18:16:09,204] [INFO] [config.py:936:print]   wall_clock_breakdown ......... False\n",
            "[2021-08-18 18:16:09,205] [INFO] [config.py:936:print]   world_size ................... 1\n",
            "[2021-08-18 18:16:09,207] [INFO] [config.py:936:print]   zero_allow_untested_optimizer  False\n",
            "[2021-08-18 18:16:09,209] [INFO] [config.py:936:print]   zero_config .................. {\n",
            "    \"stage\": 3, \n",
            "    \"contiguous_gradients\": true, \n",
            "    \"reduce_scatter\": true, \n",
            "    \"reduce_bucket_size\": 1.048576e+06, \n",
            "    \"allgather_partitions\": true, \n",
            "    \"allgather_bucket_size\": 5.000000e+08, \n",
            "    \"overlap_comm\": true, \n",
            "    \"load_from_fp32_weights\": true, \n",
            "    \"elastic_checkpoint\": false, \n",
            "    \"offload_param\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 5, \n",
            "        \"buffer_size\": 1.000000e+08, \n",
            "        \"max_in_cpu\": 1.000000e+09, \n",
            "        \"pin_memory\": true\n",
            "    }, \n",
            "    \"offload_optimizer\": {\n",
            "        \"device\": \"cpu\", \n",
            "        \"nvme_path\": null, \n",
            "        \"buffer_count\": 4, \n",
            "        \"pin_memory\": true, \n",
            "        \"pipeline_read\": false, \n",
            "        \"pipeline_write\": false, \n",
            "        \"fast_init\": false, \n",
            "        \"pipeline\": false\n",
            "    }, \n",
            "    \"sub_group_size\": 1.000000e+09, \n",
            "    \"prefetch_bucket_size\": 9.437184e+05, \n",
            "    \"param_persistence_threshold\": 1.024000e+04, \n",
            "    \"max_live_parameters\": 1.000000e+09, \n",
            "    \"max_reuse_distance\": 1.000000e+09, \n",
            "    \"gather_fp16_weights_on_model_save\": true, \n",
            "    \"ignore_unused_parameters\": true, \n",
            "    \"round_robin_gradients\": false, \n",
            "    \"legacy_stage1\": false\n",
            "}\n",
            "[2021-08-18 18:16:09,212] [INFO] [config.py:936:print]   zero_enabled ................. True\n",
            "[2021-08-18 18:16:09,213] [INFO] [config.py:936:print]   zero_optimization_stage ...... 3\n",
            "[2021-08-18 18:16:09,215] [INFO] [config.py:943:print]   json = {\n",
            "    \"fp16\": {\n",
            "        \"enabled\": true, \n",
            "        \"loss_scale\": 0, \n",
            "        \"loss_scale_window\": 1000, \n",
            "        \"initial_scale_power\": 16, \n",
            "        \"hysteresis\": 2, \n",
            "        \"min_loss_scale\": 1\n",
            "    }, \n",
            "    \"optimizer\": {\n",
            "        \"type\": \"AdamW\", \n",
            "        \"params\": {\n",
            "            \"lr\": 2e-05, \n",
            "            \"betas\": [0.9, 0.999], \n",
            "            \"eps\": 1e-08, \n",
            "            \"weight_decay\": 0.001\n",
            "        }\n",
            "    }, \n",
            "    \"scheduler\": {\n",
            "        \"type\": \"WarmupLR\", \n",
            "        \"params\": {\n",
            "            \"warmup_min_lr\": 0, \n",
            "            \"warmup_max_lr\": 2e-05, \n",
            "            \"warmup_num_steps\": 0\n",
            "        }\n",
            "    }, \n",
            "    \"zero_optimization\": {\n",
            "        \"elastic_checkpoint\": false, \n",
            "        \"stage\": 3, \n",
            "        \"offload_optimizer\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"offload_param\": {\n",
            "            \"device\": \"cpu\", \n",
            "            \"pin_memory\": true\n",
            "        }, \n",
            "        \"overlap_comm\": true, \n",
            "        \"contiguous_gradients\": true, \n",
            "        \"sub_group_size\": 1.000000e+09, \n",
            "        \"reduce_bucket_size\": 1.048576e+06, \n",
            "        \"stage3_prefetch_bucket_size\": 9.437184e+05, \n",
            "        \"stage3_param_persistence_threshold\": 1.024000e+04, \n",
            "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
            "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
            "        \"stage3_gather_fp16_weights_on_model_save\": true\n",
            "    }, \n",
            "    \"gradient_accumulation_steps\": 1, \n",
            "    \"gradient_clipping\": 1.0, \n",
            "    \"steps_per_print\": 2.000000e+03, \n",
            "    \"train_batch_size\": 1, \n",
            "    \"train_micro_batch_size_per_gpu\": 1, \n",
            "    \"wall_clock_breakdown\": false\n",
            "}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 6\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 1\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 6\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Using /root/.cache/torch_extensions as PyTorch extensions root...\n",
            "No modifications detected for re-loaded extension module utils, skipping build step...\n",
            "Loading extension module utils...\n",
            "Time to load utils op: 0.007422924041748047 seconds\n",
            "[2021-08-18 18:16:10,575] [INFO] [stage3.py:2735:_overflow_clean_up] [deepscale] OVERFLOW! Rank 0 Skipping step. Attempted loss scale: 65536, reducing to 65536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:08, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6, training_loss=nan, metrics={'train_runtime': 9.58, 'train_samples_per_second': 0.626, 'train_steps_per_second': 0.626, 'total_flos': 6473952.0, 'train_loss': nan, 'epoch': 1.0})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf3cmC6PzKio",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a971a678-0694-43cf-aedf-f41a8b96a080"
      },
      "source": [
        "model_name=\"DS-config-5\"\n",
        "tokenizer.push_to_hub(model_name)\n",
        "model.push_to_hub(model_name)\n",
        "\n",
        "# config1 = BartConfig(name_or_path=model_name, min_length=0, max_length=1024, num_beams=2, length_penalty=1, early_stopping=True)\n",
        "# config1.push_to_hub(model_name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6d5fdace0167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DS-config-5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# config1 = BartConfig(name_or_path=model_name, min_length=0, max_length=1024, num_beams=2, length_penalty=1, early_stopping=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "R_hgdC42nEM5",
        "outputId": "07280f00-c8c4-4251-9d35-ad9fd9e57f93"
      },
      "source": [
        "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt', truncation=True)\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs['input_ids'])\n",
        "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d346b6946b76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Generate Summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msummary_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msummary_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_ids, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, repetition_penalty, bad_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m    920\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0;31m# add encoder_outputs to model_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 922\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_encoder_decoder_kwargs_for_generation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m             \u001b[0;31m# set input_ids as decoder_input_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, input_ids, model_kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"decoder_\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0margument\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cross_attn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             }\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bart/modeling_bart.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0membed_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_positions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1071\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    158\u001b[0m         return F.embedding(\n\u001b[1;32m    159\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2041\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2042\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2043\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2045\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking arugment for argument index in method wrapper_index_select)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bg3so4q-VPyE"
      },
      "source": [
        "model_name = \"Pyke/DS-config-5\"\n",
        "# config = BartConfig.from_pretrained(model_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CvrhCtvaoka"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSklkzKCn6Cy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "fa47e0f80ea3427eb6c4de0db7e98314",
            "28c279c001af4691946e698dab62d4c4",
            "fe6d4ed4f361478c912c0a935485119e",
            "f0a10bd7b6334485a728bdaab889057a",
            "00eacc960c4a4d199088f2b238c467a3",
            "3dbcc45ea92048eba5de012a6e74fb0a",
            "945becf7d1c94c209ac09d67105c48bd",
            "9e75ba8660924cf8b102d12c7c3018a5",
            "ddcb3417c1764dc98716690babd4c403",
            "2a085688c6e6494a9779c21159fe9da3",
            "b8ae1f5d2f2d4ac096deab02d80cdaf7",
            "760a4915b5814f099f65284f7dca778d",
            "8be67e6b9dac4aafa98d479d889ca750",
            "907cec2dd19a420bbfb989cac8a869c7",
            "eb94f18d22ad4aaaa871ba1e5c9ab298",
            "f20433a131d44eebbf417d5a523b2d3a",
            "ba35cc2b6ae045d9bd9a4129a93100a9",
            "fe56386302ba4f05948999439d2932e2",
            "8c169934f016401089346319f5523121",
            "7924d7449346465aa8e7a9855108b51f",
            "4aeec6e32c8943e1b5df9def99d58968",
            "3d09d777a31c4d399b30d6f40ae66ad5"
          ]
        },
        "outputId": "243d72f3-a514-4eb7-ff43-ace628315121"
      },
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = \"My friends are cool but they eat too many carbs.\"\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors='pt')\n",
        "\n",
        "# Generate Summary\n",
        "summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=10, early_stopping=True)\n",
        "print([tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in summary_ids])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa47e0f80ea3427eb6c4de0db7e98314",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.64k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "760a4915b5814f099f65284f7dca778d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e5afe5ff8506>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartForConditionalGeneration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBartTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mARTICLE_TO_SUMMARIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"My friends are cool but they eat too many carbs.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mARTICLE_TO_SUMMARIZE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1359\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m                 \u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_mismatched_sizes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m                 \u001b[0m_fast_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_fast_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m             )\n\u001b[1;32m   1363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_model\u001b[0;34m(cls, model, state_dict, pretrained_model_name_or_path, ignore_mismatched_sizes, _fast_init)\u001b[0m\n\u001b[1;32m   1500\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\\t\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Error(s) in loading state_dict for {model.__class__.__name__}:\\n\\t{error_msg}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for BartForConditionalGeneration:\n\tsize mismatch for model.shared.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([50264, 1024]).\n\tsize mismatch for model.encoder.embed_tokens.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([50264, 1024]).\n\tsize mismatch for model.encoder.embed_positions.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1026, 1024]).\n\tsize mismatch for model.encoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.0.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.0.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.1.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.1.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.2.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.2.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.3.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.3.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.4.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.4.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.5.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.5.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.6.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.6.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.7.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.7.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.8.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.8.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.9.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.9.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.10.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.10.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.encoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.encoder.layers.11.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.encoder.layers.11.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.embed_tokens.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([50264, 1024]).\n\tsize mismatch for model.decoder.embed_positions.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1026, 1024]).\n\tsize mismatch for model.decoder.layers.0.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.0.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.0.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.1.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.1.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.1.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.2.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.2.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.2.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.3.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.3.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.3.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.4.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.4.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.4.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.5.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.5.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.5.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.6.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.6.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.6.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.7.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.7.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.7.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.8.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.8.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.8.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.9.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.9.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.9.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.10.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.10.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.10.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.decoder.layers.11.self_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.self_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.self_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.self_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.encoder_attn.k_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.encoder_attn.v_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.encoder_attn.q_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.encoder_attn.out_proj.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.decoder.layers.11.fc1.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.decoder.layers.11.fc2.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for lm_head.weight: copying a param with shape torch.Size([1]) from checkpoint, the shape in current model is torch.Size([50264, 1024])."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOM9KlHAMu8S"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}